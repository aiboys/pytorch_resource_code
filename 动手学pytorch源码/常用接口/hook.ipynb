{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "https://zhuanlan.zhihu.com/p/75054200\n## 1. 中间梯度的查询\n\n在 PyTorch 的计算图（computation graph）中，只有叶子结点（leaf nodes）的变量会保留梯度。\n而所有中间变量的梯度只被用于反向传播，一旦完成反向传播，中间变量的梯度就将自动释放，从而节约内存。\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## 1.1 两种方式实现查询：\n1) 在反向传播前使用retain_grad -- 该方法耗内存\n2) 使用hook机制 -- 推荐:  hook_fn(grad) -\u003e Tensor or None\n\n注意: 虽然是不同的实现方式,但是对于节点来说两者都是注册在`Tensor._backward_hooks`字典里\n参考: [完全理解Pytorch里面的Hook机制教学视频](https://www.bilibili.com/video/BV1MV411t7td?from\u003dsearch\u0026seid\u003d4733570378288382581)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% import torch\n",
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "x.requires_grad: True\ny.requires_grad: True\nz.requires_grad: True\nw.requires_grad: True\no.requires_grad: True\nx.grad: tensor([1., 2., 3., 4.])\ny.grad: tensor([1., 2., 3., 4.])\nw.grad: tensor([ 4.,  6.,  8., 10.])\nz.grad: None\no.grad: None\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\n\n\nimport torch\n\nx \u003d torch.Tensor([0, 1, 2, 3]).requires_grad_()\ny \u003d torch.Tensor([4, 5, 6, 7]).requires_grad_()\nw \u003d torch.Tensor([1, 2, 3, 4]).requires_grad_()\nz \u003d x+y\n# z.retain_grad()\n\no \u003d w.matmul(z)\no.backward()  \n# o.retain_grad()\n\nprint(\u0027x.requires_grad:\u0027, x.requires_grad) # True\nprint(\u0027y.requires_grad:\u0027, y.requires_grad) # True\nprint(\u0027z.requires_grad:\u0027, z.requires_grad) # True\nprint(\u0027w.requires_grad:\u0027, w.requires_grad) # True\nprint(\u0027o.requires_grad:\u0027, o.requires_grad) # True\n\n\nprint(\u0027x.grad:\u0027, x.grad) # tensor([1., 2., 3., 4.])\nprint(\u0027y.grad:\u0027, y.grad) # tensor([1., 2., 3., 4.])\nprint(\u0027w.grad:\u0027, w.grad) # tensor([ 4.,  6.,  8., 10.])\nprint(\u0027z.grad:\u0027, z.grad) # None\nprint(\u0027o.grad:\u0027, o.grad) # None"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "x.requires_grad: True\ny.requires_grad: True\nz.requires_grad: True\nw.requires_grad: True\no.requires_grad: True\nx.grad: tensor([1., 2., 3., 4.])\ny.grad: tensor([1., 2., 3., 4.])\nw.grad: tensor([ 4.,  6.,  8., 10.])\nz.grad: tensor([1., 2., 3., 4.])\no.grad: tensor(1.)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# retain_grad\n\nimport torch\n\nx \u003d torch.Tensor([0, 1, 2, 3]).requires_grad_()\ny \u003d torch.Tensor([4, 5, 6, 7]).requires_grad_()\nw \u003d torch.Tensor([1, 2, 3, 4]).requires_grad_()\nz \u003d x+y\nz.retain_grad()\n\no \u003d w.matmul(z)\no.retain_grad()  # 在反向传播前进行retain_grad就可以得到保留梯度，但是这种方式很耗内存\no.backward()\n\n\nprint(\u0027x.requires_grad:\u0027, x.requires_grad) # True\nprint(\u0027y.requires_grad:\u0027, y.requires_grad) # True\nprint(\u0027z.requires_grad:\u0027, z.requires_grad) # True\nprint(\u0027w.requires_grad:\u0027, w.requires_grad) # True\nprint(\u0027o.requires_grad:\u0027, o.requires_grad) # True\n\n\nprint(\u0027x.grad:\u0027, x.grad) # tensor([1., 2., 3., 4.])\nprint(\u0027y.grad:\u0027, y.grad) # tensor([1., 2., 3., 4.])\nprint(\u0027w.grad:\u0027, w.grad) # tensor([ 4.,  6.,  8., 10.])\nprint(\u0027z.grad:\u0027, z.grad) # None\nprint(\u0027o.grad:\u0027, o.grad) # None",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "OrderedDict([(22, \u003cfunction Tensor.retain_grad.\u003clocals\u003e.retain_grad_hook at 0x0000024BB06A6F28\u003e), (23, \u003cfunction hook_fn at 0x0000024BB06F27B8\u003e)])\n\u003d\u003d\u003d\u003d\u003dStart backprop\u003d\u003d\u003d\u003d\u003d\ngrad:  tensor(1.)\ngrad:  tensor([1., 2., 3., 4.])\n\u003d\u003d\u003d\u003d\u003dEnd backprop\u003d\u003d\u003d\u003d\u003d\nx.grad: tensor([1., 2., 3., 4.])\ny.grad: tensor([1., 2., 3., 4.])\nw.grad: tensor([ 4.,  6.,  8., 10.])\nz.grad: tensor([1., 2., 3., 4.])\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\n# 使用hook\nimport torch\n\nx \u003d torch.Tensor([0, 1, 2, 3]).requires_grad_()\ny \u003d torch.Tensor([4, 5, 6, 7]).requires_grad_()\nw \u003d torch.Tensor([1, 2, 3, 4]).requires_grad_()\nz \u003d x+y\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\ndef hook_fn(grad):\n    print(\"grad: \", grad)\n\nz.retain_grad()\nzhook \u003d z.register_hook(hook_fn)\nz.retain_grad()\n\nprint(z._backward_hooks)  \n## 注意: 尽管上面的z有三个hook,但是每一个hook加入的时候都是按照字典的规则进行加入的,即\n##  不会重复添加同一种hook\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\no \u003d w.matmul(z)\nohook \u003d o.register_hook(hook_fn)\n\nprint(\u0027\u003d\u003d\u003d\u003d\u003dStart backprop\u003d\u003d\u003d\u003d\u003d\u0027)\no.backward()  # 此时hook自动执行\nprint(\u0027\u003d\u003d\u003d\u003d\u003dEnd backprop\u003d\u003d\u003d\u003d\u003d\u0027)\n\nprint(\u0027x.grad:\u0027, x.grad)\nprint(\u0027y.grad:\u0027, y.grad)\nprint(\u0027w.grad:\u0027, w.grad)\nprint(\u0027z.grad:\u0027, z.grad)\n\nzhook.remove()\nohook.remove()  # 在使用完hook后别忘记了释放掉,除非后续还要用",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## 2. hook改变中间梯度值",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\u003d\u003d\u003d\u003d\u003dStart backprop\u003d\u003d\u003d\u003d\u003d\ntensor([2., 4., 6., 8.])\n\u003d\u003d\u003d\u003d\u003dEnd backprop\u003d\u003d\u003d\u003d\u003d\nx.grad: tensor([2., 4., 6., 8.])\ny.grad: tensor([2., 4., 6., 8.])\nw.grad: tensor([ 4.,  6.,  8., 10.])\nz.grad: None\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nimport torch\nimport torch\n\nx \u003d torch.Tensor([0, 1, 2, 3]).requires_grad_()\ny \u003d torch.Tensor([4, 5, 6, 7]).requires_grad_()\nw \u003d torch.Tensor([1, 2, 3, 4]).requires_grad_()\nz \u003d x + y\n\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\ndef hook_fn(grad):\n    g \u003d 2 * grad\n    print(g)\n    return g\n\n\nz.register_hook(hook_fn)\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\no \u003d w.matmul(z)\n\nprint(\u0027\u003d\u003d\u003d\u003d\u003dStart backprop\u003d\u003d\u003d\u003d\u003d\u0027)\no.backward()\nprint(\u0027\u003d\u003d\u003d\u003d\u003dEnd backprop\u003d\u003d\u003d\u003d\u003d\u0027)\n\nprint(\u0027x.grad:\u0027, x.grad)\nprint(\u0027y.grad:\u0027, y.grad)\nprint(\u0027w.grad:\u0027, w.grad)\nprint(\u0027z.grad:\u0027, z.grad)  # 因为z的梯度变为两倍，因此反传的时候前面的梯度也是2倍\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.0.1 不要再hook里对grad做inplace操作\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "tensor(3.)\ntensor(2.)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nimport torch\na \u003d torch.tensor(2., requires_grad\u003dTrue)\nb \u003d torch.tensor(3., requires_grad\u003dTrue)\n\nc \u003d a*b \nd \u003d torch.tensor(4., requires_grad\u003dTrue)\n\ndef d_hook(grad):\n    \n    grad *\u003d100  # 这个grad会影响到c的梯度，从而影响整个grad传播\n    \n    # return grad * 100  # 这样就不会影响a,b的梯度\nd.register_hook(d_hook)\n\ne \u003d c + d\ne.backward()\n\nprint(a.grad)  # 300\nprint(b.grad)  # 200\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.1 一个变量同时绑定多个hook",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\u003d\u003d\u003d\u003d\u003dStart backprop\u003d\u003d\u003d\u003d\u003d\ngrad:  tensor([101., 102., 103., 104.])\n\u003d\u003d\u003d\u003d\u003dEnd backprop\u003d\u003d\u003d\u003d\u003d\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import torch\n\nx \u003d torch.Tensor([0, 1, 2, 3]).requires_grad_()\ny \u003d torch.Tensor([4, 5, 6, 7]).requires_grad_()\nw \u003d torch.Tensor([1, 2, 3, 4]).requires_grad_()\nz \u003d x + y\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nz.register_hook(lambda x: 100.+ x)\nz.register_hook(lambda x: print(\"grad: \", x))\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\no \u003d w.matmul(z)\n\nprint(\u0027\u003d\u003d\u003d\u003d\u003dStart backprop\u003d\u003d\u003d\u003d\u003d\u0027)\no.backward()\nprint(\u0027\u003d\u003d\u003d\u003d\u003dEnd backprop\u003d\u003d\u003d\u003d\u003d\u0027)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## 3. hook for Module\n\n网络模块 module 不像上一节中的 Tensor，拥有显式的变量名可以直接访问，而是被封装在神经网络中间。我们通常只能获得网络整体的输入和输出，对于夹在网络中间的模块，我们不但很难得知它输入/输出的梯度，甚至连它输入输出的数值都无法获得。除非设计网络时，在 forward 函数的返回值中包含中间 module 的输出，或者用很麻烦的办法，把网络按照 module 的名称拆分再组合，让中间层提取的 feature 暴露出来。\n\n为了解决这个麻烦，PyTorch 设计了两种 hook：register_forward_hook 和 register_backward_hook，分别用来获取正/反向传播时，中间层模块输入和输出的 feature/gradient，大大降低了获取模型内部信息流的难度。\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "### 3.2. register pre forward hook\n\n在forward之前的hook\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md \n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "(tensor(2., requires_grad\u003dTrue), tensor(3., requires_grad\u003dTrue), tensor(4., requires_grad\u003dTrue))\npre b: tensor(3., requires_grad\u003dTrue)\n"
          ],
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-22-297231851ff2\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 48\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0msum_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mh:\\anaconda3\\envs2\\pytorch1.5\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: \u0027c\u0027"
          ],
          "ename": "TypeError",
          "evalue": "forward() missing 1 required positional argument: \u0027c\u0027",
          "output_type": "error"
        }
      ],
      "source": "import torch\nimport torch.nn  as nn\n\nclass sumnet(nn.Module):\n    def __init__(self):\n        super(sumnet, self).__init__()\n    \n    @staticmethod\n    def forward(a,b,c):\n        d \u003d a+b+c\n        print(\"a:\", a)\n        print(\"b:\",b)\n        print(\"c:\",c)\n        return d\n\ndef forward_pre_hook(module, input):\n    \u0027\u0027\u0027\n    The input contains only the positional arguments given to the module. \n    Keyword arguments won’t be passed to the hooks and only to the forward. \n    The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value\n     into a tuple if a single value is returned(unless that value is already a tuple).\n    \n    :param module: \n    :param input: \n    :return: \n    \u0027\u0027\u0027\n    a,b \u003d input  \n    \n    print(\"pre b:\", b)\n    \n    return a+10, b # 经过pre_hook后，输入forward的是a \u003d 12, b \u003d 3, c \u003d 4.\n\n\n\ndef forward_hook(module, inputs, outputs):\n    \u0027\u0027\u0027\n    \n    :param module: \n    :param inputs: 接受来自pre_forward_hook的输出！！！ 即只有a, b\n    :param outputs: 此时的outputs接受的输入是forward的输出结果\n    :return: 返回的值会覆盖forward的输出，送到下一个module里执行\n    \u0027\u0027\u0027\n    \n    print(\"inputs: \",inputs)  # 12\n    print(\"outputs: \",outputs) # 19\n    return outputs+100  \n\nsum_net \u003d sumnet()\n\nsum_net.register_forward_pre_hook(forward_pre_hook)\nsum_net.register_forward_hook(forward_hook)\n\na \u003d torch.tensor(2., requires_grad\u003dTrue)\nb \u003d torch.tensor(3., requires_grad\u003dTrue)\nc \u003d torch.tensor(4., requires_grad\u003dTrue)\n\nd \u003d sum_net(a,b,c\u003d c)\nd.backward()\nprint(d)\n\n        \n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 3.1. register forward hook\n在forward之后的hook\n\nregister_forward_hook的作用是获取前向传播过程中，各个网络模块的输入和输出。\n对于模块 module，其使用方式为：module.register_forward_hook(hook_fn)\nhook_fn(module, input, output) -\u003e None\n\n它的输入变量分别为：模块，模块的输入，模块的输出，和对 Tensor 的 hook 不同，forward hook 不返回任何值，\n也就是说不能用它来修改输入或者输出的值（注意：从 pytorch 1.2.0 开始，forward hook 也有返回值了，可以修改网络模块的输出），\n但借助这个 hook，我们可以方便地用预训练的神经网络提取特征，而不用改变预训练网络的结构。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Linear(in_features\u003d3, out_features\u003d4, bias\u003dTrue)\ninput (tensor([[1., 1., 1.]], requires_grad\u003dTrue),)\noutput tensor([[  7., -13.,  27., -29.]], grad_fn\u003d\u003cAddmmBackward\u003e)\nReLU()\ninput (tensor([[  7., -13.,  27., -29.]], grad_fn\u003d\u003cAddmmBackward\u003e),)\noutput tensor([[ 7.,  0., 27.,  0.]], grad_fn\u003d\u003cReluBackward0\u003e)\nLinear(in_features\u003d4, out_features\u003d1, bias\u003dTrue)\ninput (tensor([[ 7.,  0., 27.,  0.]], grad_fn\u003d\u003cReluBackward0\u003e),)\noutput tensor([[89.]], grad_fn\u003d\u003cAddmmBackward\u003e)\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003dSaved inputs and outputs\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\ninput:  (tensor([[1., 1., 1.]], requires_grad\u003dTrue),)\noutput:  tensor([[  7., -13.,  27., -29.]], grad_fn\u003d\u003cAddmmBackward\u003e)\ninput:  (tensor([[  7., -13.,  27., -29.]], grad_fn\u003d\u003cAddmmBackward\u003e),)\noutput:  tensor([[ 7.,  0., 27.,  0.]], grad_fn\u003d\u003cReluBackward0\u003e)\ninput:  (tensor([[ 7.,  0., 27.,  0.]], grad_fn\u003d\u003cReluBackward0\u003e),)\noutput:  tensor([[89.]], grad_fn\u003d\u003cAddmmBackward\u003e)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import torch\nfrom torch import nn\n\n# 首先我们定义一个模型\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc1 \u003d nn.Linear(3, 4)\n        self.relu1 \u003d nn.ReLU()\n        self.fc2 \u003d nn.Linear(4, 1)\n        self.initialize()\n    \n    # 为了方便验证，我们将指定特殊的weight和bias\n    def initialize(self):\n        with torch.no_grad():\n            self.fc1.weight \u003d torch.nn.Parameter(\n                torch.Tensor([[1., 2., 3.],\n                              [-4., -5., -6.],\n                              [7., 8., 9.],\n                              [-10., -11., -12.]]))\n\n            self.fc1.bias \u003d torch.nn.Parameter(torch.Tensor([1.0, 2.0, 3.0, 4.0]))\n            self.fc2.weight \u003d torch.nn.Parameter(torch.Tensor([[1.0, 2.0, 3.0, 4.0]]))\n            self.fc2.bias \u003d torch.nn.Parameter(torch.Tensor([1.0]))\n\n    def forward(self, x):\n        o \u003d self.fc1(x)\n        o \u003d self.relu1(o)\n        o \u003d self.fc2(o)\n        return o\n\n# 全局变量，用于存储中间层的 feature\ntotal_feat_out \u003d []\ntotal_feat_in \u003d []\n\n# 定义 forward hook function\ndef hook_fn_forward(module, input, output):\n    print(module) # 用于区分模块\n    print(\u0027input\u0027, input) # 首先打印出来\n    print(\u0027output\u0027, output)\n    total_feat_out.append(output) # 然后分别存入全局 list 中\n    total_feat_in.append(input)\n\n\nmodel \u003d Model()\n\nmodules \u003d model.named_children() # \nfor name, module in modules:\n    module.register_forward_hook(hook_fn_forward)\n\n# 注意下面代码中 x 的维度，对于linear module，输入一定是大于等于二维的\n# （第一维是 batch size）。\n\nx \u003d torch.Tensor([[1.0, 1.0, 1.0]]).requires_grad_() \no \u003d model(x)\no.backward()\n\nprint(\u0027\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003dSaved inputs and outputs\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u0027)\nfor idx in range(len(total_feat_in)):\n    print(\u0027input: \u0027, total_feat_in[idx])\n    print(\u0027output: \u0027, total_feat_out[idx])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n### 3.2. register backward hook (目前还有bug，不推荐使用)\n\n* 1) 和 register_forward_hook相似，register_backward_hook 的作用是获取神经网络反向传播过程中，\n各个模块输入端和输出端的梯度值。对于模块 module，其使用方式为：module.register_backward_hook(hook_fn) \n\n* 2) 其中hook_fn的函数签名为：\n\n`hook_fn(module, grad_input, grad_output) -\u003e Tensor or None`\n\n它的输入变量分别为：模块，模块输入端的梯度，模块输出端的梯度。\n需要注意的是，这里的输入端和输出端，是站在**前向传播**的角度的，而不是反向传播的角度。\n例如线性模块：o\u003dW*x+b，其输入端为 W，x 和 b，输出端为 o.\n\n* 3)如果模块有多个输入或者输出的话，grad_input和grad_output可以是 tuple 类型。\n对于线性模块：o\u003dW*x+b ，**它的输入端包括了W、x 和 b 三部分，\n因此 grad_input 就是一个包含三个元素的 tuple.**\n（注意，这里的输入和输出是相对于该模块的，类似的比如在torch.autograd.function.Function\n重新定义模块的self.backward的输入就是grad_output，这个是output是上一层对该层的梯度输入，即视角都是前向传播的视角，\ngrad_input就是该模块梯度输出到上一层模块进行上模块的反传）\n\n和forward hook不同点：\n\n    1. 在forward hook中，hook_fn的输入是x; 不包括w, b, 但是backward hook包括，输入和输出是元组\n    2. 返回tensor或者None, backward hook函数不能直接改变它的输入变量, 但是可以返回新的grad_input,反向传播到它上一个模块\n        ） ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Linear(in_features\u003d4, out_features\u003d1, bias\u003dTrue)\ngrad_output (tensor([[1.]]),)\ngrad_input (tensor([1.]), tensor([[1., 2., 3., 4.]]), tensor([[ 7.],\n        [ 0.],\n        [27.],\n        [ 0.]]))\nReLU()\ngrad_output (tensor([[1., 2., 3., 4.]]),)\ngrad_input (tensor([[1., 0., 3., 0.]]),)\nLinear(in_features\u003d3, out_features\u003d4, bias\u003dTrue)\ngrad_output (tensor([[1., 0., 3., 0.]]),)\ngrad_input (tensor([1., 0., 3., 0.]), tensor([[22., 26., 30.]]), tensor([[1., 0., 3., 0.],\n        [1., 0., 3., 0.],\n        [1., 0., 3., 0.]]))\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003dSaved inputs and outputs\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\ngrad output:  (tensor([[1.]]),)\ngrad input:  (tensor([1.]), tensor([[1., 2., 3., 4.]]), tensor([[ 7.],\n        [ 0.],\n        [27.],\n        [ 0.]]))\ngrad output:  (tensor([[1., 2., 3., 4.]]),)\ngrad input:  (tensor([[1., 0., 3., 0.]]),)\ngrad output:  (tensor([[1., 0., 3., 0.]]),)\ngrad input:  (tensor([1., 0., 3., 0.]), tensor([[22., 26., 30.]]), tensor([[1., 0., 3., 0.],\n        [1., 0., 3., 0.],\n        [1., 0., 3., 0.]]))\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import torch\nfrom torch import nn\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc1 \u003d nn.Linear(3, 4)\n        self.relu1 \u003d nn.ReLU()\n        self.fc2 \u003d nn.Linear(4, 1)\n        self.initialize()\n\n    def initialize(self):\n        with torch.no_grad():\n            self.fc1.weight \u003d torch.nn.Parameter(\n                torch.Tensor([[1., 2., 3.],\n                              [-4., -5., -6.],\n                              [7., 8., 9.],\n                              [-10., -11., -12.]]))\n\n            self.fc1.bias \u003d torch.nn.Parameter(torch.Tensor([1.0, 2.0, 3.0, 4.0]))\n            self.fc2.weight \u003d torch.nn.Parameter(torch.Tensor([[1.0, 2.0, 3.0, 4.0]]))\n            self.fc2.bias \u003d torch.nn.Parameter(torch.Tensor([1.0]))\n\n    def forward(self, x):\n        o \u003d self.fc1(x)\n        o \u003d self.relu1(o)\n        o \u003d self.fc2(o)\n        return o\n\n\ntotal_grad_out \u003d []\ntotal_grad_in \u003d []\n\n\ndef hook_fn_backward(module, grad_input, grad_output):\n    print(module) # 为了区分模块\n    # 为了符合反向传播的顺序，我们先打印 grad_output\n    print(\u0027grad_output\u0027, grad_output) \n    # 再打印 grad_input\n    print(\u0027grad_input\u0027, grad_input)\n    # 保存到全局变量\n    total_grad_in.append(grad_input)\n    total_grad_out.append(grad_output)\n\n\nmodel \u003d Model()\n\nmodules \u003d model.named_children()\nfor name, module in modules:\n    module.register_backward_hook(hook_fn_backward)\n\n# 这里的 requires_grad 很重要，如果不加，backward hook\n# 执行到第一层，对 x 的导数将为 None，某英文博客作者这里疏忽了\n# 此外再强调一遍 x 的维度，一定不能写成 torch.Tensor([1.0, 1.0, 1.0]).requires_grad_()\n# 否则 backward hook 会出问题。\nx \u003d torch.Tensor([[1.0, 1.0, 1.0]]).requires_grad_()\no \u003d model(x)\no.backward()\n\nprint(\u0027\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003dSaved inputs and outputs\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u0027)\nfor idx in range(len(total_grad_in)):\n    print(\u0027grad output: \u0027, total_grad_out[idx])\n    print(\u0027grad input: \u0027, total_grad_in[idx])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n### 注意事项\n\nregister_backward_hook只能操作简单模块，而不能操作包含多个子模块的复杂模块。 \n如果对复杂模块用了 backward hook，那么我们只能得到该模块最后一次简单操作的梯度信息。\n对于上面的代码稍作修改，不再遍历各个子模块，\n而是把 model 整体绑在一个 hook_fn_backward上\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}