{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "\n# 模型保存以及加载等一系列问题\n\n参考 [save, load](https://zhuanlan.zhihu.com/p/107203828), [module/optimizer.state_dict](https://zhuanlan.zhihu.com/p/84797438)\n[nn.Module](https://zhuanlan.zhihu.com/p/340453841)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": "\n## 1. torch.nn.Module.state_dict\n\nModule.state_dict()通过调用self._save_to_state_dict()将模型的self._parameters, self._buffers保存\n进一个Orderdict中.\n\nOptimizer.state_dict()返回 `{\n            \u0027state\u0027: packed_state,\n            \u0027param_groups\u0027: param_groups,\n        }`"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.001\n    momentum: 0.9\n    nesterov: False\n    weight_decay: 0\n)\nModel\u0027s state_dict:\nodict_keys([\u0027conv1.weight\u0027, \u0027conv1.bias\u0027, \u0027conv2.weight\u0027, \u0027conv2.bias\u0027, \u0027fc1.weight\u0027, \u0027fc1.bias\u0027, \u0027fc2.weight\u0027, \u0027fc2.bias\u0027, \u0027fc3.weight\u0027, \u0027fc3.bias\u0027])\nconv1.weight \t torch.Size([6, 3, 5, 5])\nconv1.bias \t torch.Size([6])\nconv2.weight \t torch.Size([16, 6, 5, 5])\nconv2.bias \t torch.Size([16])\nfc1.weight \t torch.Size([120, 400])\nfc1.bias \t torch.Size([120])\nfc2.weight \t torch.Size([84, 120])\nfc2.bias \t torch.Size([84])\nfc3.weight \t torch.Size([10, 84])\nfc3.bias \t torch.Size([10])\nOptimizer\u0027s state_dict:\nstate \t {}\nparam_groups \t [{\u0027lr\u0027: 0.001, \u0027momentum\u0027: 0.9, \u0027dampening\u0027: 0, \u0027weight_decay\u0027: 0, \u0027nesterov\u0027: False, \u0027params\u0027: [3176176701872, 3176176702016, 3176174379944, 3176176482632, 3176176482128, 3176176481696, 3176176482848, 3176176481552, 3176176482920, 3176176481048]}]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\nimport torch.optim as optim\nclass TheModelClass(nn.Module):\n    def __init__(self):\n        super(TheModelClass, self).__init__()\n        self.conv1 \u003d nn.Conv2d(3, 6, 5)\n        self.pool \u003d nn.MaxPool2d(2, 2)\n        self.conv2 \u003d nn.Conv2d(6, 16, 5)\n        self.fc1 \u003d nn.Linear(16 * 5 * 5, 120)\n        self.fc2 \u003d nn.Linear(120, 84)\n        self.fc3 \u003d nn.Linear(84, 10)\n\n    def forward(self, x):\n        x \u003d self.pool(F.relu(self.conv1(x)))\n        x \u003d self.pool(F.relu(self.conv2(x)))\n        x \u003d x.view(-1, 16 * 5 * 5)\n        x \u003d F.relu(self.fc1(x))\n        x \u003d F.relu(self.fc2(x))\n        x \u003d self.fc3(x)\n        return x\n\n# Initialize model\nmodel \u003d TheModelClass()\n\n# Initialize optimizer\noptimizer \u003d optim.SGD(model.parameters(), lr\u003d0.001, momentum\u003d0.9)\nprint(optimizer)\n# Print model\u0027s state_dict\nprint(\"Model\u0027s state_dict:\")\nprint(model.state_dict().keys())\nfor param_tensor in model.state_dict():\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n\n# Print optimizer\u0027s state_dict\nprint(\"Optimizer\u0027s state_dict:\")\nfor var_name in optimizer.state_dict():\n    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": "\n## 2. torch.save\n\n将字典数据序列化保存为二进制文件"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": "\n### resource code\n\n在 torch.serialization.save()里源码如下:\n\n```\ndef save(obj, f, pickle_module\u003dpickle, pickle_protocol\u003dDEFAULT_PROTOCOL):\n    # 其中obj一般是字典格式数据, f为`.pth, .pt`格式的二进制压缩文件, pickle_module默认为pickle\n    # \n    return _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\n```\n\n\n其中`_with_file_like`源码如下:\n\n```\ndef _with_file_like(f, mode, body):\n    \"\"\"\n    Executes a body function with a file object for f, opening\n    it in \u0027mode\u0027 if it is a string filename.\n    \"\"\"\n    new_fd \u003d False\n    if isinstance(f, str) or \\\n            (sys.version_info[0] \u003d\u003d 2 and isinstance(f, unicode)) or \\\n            (sys.version_info[0] \u003d\u003d 3 and isinstance(f, pathlib.Path)):\n        new_fd \u003d True\n        f \u003d open(f, mode)\n    try:\n        return body(f)\n    finally:\n        if new_fd:\n            f.close()\n```\n\n```\ndef save(obj, f, pickle_module\u003dpickle, pickle_protocol\u003dDEFAULT_PROTOCOL):\n\n    pickle_module.dump(MAGIC_NUMBER, f, protocol\u003dpickle_protocol)\n    pickle_module.dump(PROTOCOL_VERSION, f, protocol\u003dpickle_protocol)\n    pickle_module.dump(sys_info, f, protocol\u003dpickle_protocol)\n    pickler \u003d pickle_module.Pickler(f, protocol\u003dpickle_protocol)\n    pickler.persistent_id \u003d persistent_id\n    pickler.dump(obj)\n\n    serialized_storage_keys \u003d sorted(serialized_storages.keys())\n    pickle_module.dump(serialized_storage_keys, f, protocol\u003dpickle_protocol)\n    f.flush()\n\n```\n\n**可以看到其实整个保存过程就是利用的pickle序列化工具来保存字典数据**\n\n#### example\n\n```\ntorch.save({\n            \u0027epoch\u0027: epoch,\n            \u0027model_state_dict\u0027: model.state_dict(),  # dict([\u0027weight\u0027: torch.tensor, \u0027bias\u0027: torch.tensor])\n            \u0027optimizer_state_dict\u0027: optimizer.state_dict(), # 一般也会把optimizer的有关优化器状态以及所用超参数的信息保存\n            \u0027loss\u0027: loss,\n            ...\n            }, PATH)\n```\n"
    },
    {
      "cell_type": "markdown",
      "source": "\n## 3. torch.load\n\n反序列化 `torch.load(\u0027.pth\u0027)`\n\n`load(f, map_location\u003dNone, pickle_module\u003dpickle, **pickle_load_args)` 加载从`torch.save`\n保存的二进制文件. 利用python的pickle反序列化pickle.load来对序列化的文件进行加载以及反序列化为字典数据.\n\n**关于其中的map_location参数**\n    * `map_location`参数接受两个参数: `storage`\u0026`location` \n    ```\n    # If map_location is a callable, it will be called once for each serialized storage with two arguments: storage and location. \n    # The storage argument will be the initial deserialization of the storage, residing on the CPU. \n        Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to map_location. \n        The builtin location tags are \u0027cpu\u0027 for CPU tensors and \u0027cuda:device_id\u0027 (e.g. \u0027cuda:2\u0027) for CUDA tensors. map_location should return either None or a storage. \n        If map_location returns a storage, it will be used as the final deserialized object, already moved to the right device. \n        Otherwise, torch.load() will fall back to the default behavior, as if map_location wasn’t specified.\n\n    # If map_location is a torch.device object or a string containing a device tag, it indicates the location where all tensors should be loaded.\n\n    # Otherwise, if map_location is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values).```\n    \n    # When you call torch.load() on a file which contains GPU tensors, those tensors will be loaded to GPU by default. \n        You can call torch.load(.., map_location\u003d\u0027cpu\u0027) and then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint.\n#### Example: \n\n        \u003e\u003e\u003e torch.load(\u0027tensors.pt\u0027)\n        # Load all tensors onto the CPU\n        \u003e\u003e\u003e torch.load(\u0027tensors.pt\u0027, map_location\u003dtorch.device(\u0027cpu\u0027))\n        # Load all tensors onto the CPU, using a function\n        \u003e\u003e\u003e torch.load(\u0027tensors.pt\u0027, map_location\u003dlambda storage, loc: storage)\n        # Load all tensors onto GPU 1\n        \u003e\u003e\u003e torch.load(\u0027tensors.pt\u0027, map_location\u003dlambda storage, loc: storage.cuda(1))\n        # Map tensors from GPU 1 to GPU 0\n        \u003e\u003e\u003e torch.load(\u0027tensors.pt\u0027, map_location\u003d{\u0027cuda:1\u0027:\u0027cuda:0\u0027})\n        # Load tensor from io.BytesIO object\n        \u003e\u003e\u003e with open(\u0027tensor.pt\u0027, \u0027rb\u0027) as f:\n                buffer \u003d io.BytesIO(f.read())\n        \u003e\u003e\u003e torch.load(buffer)\n        # Load a module with \u0027ascii\u0027 encoding for unpickling\n        \u003e\u003e\u003e torch.load(\u0027module.pt\u0027, encoding\u003d\u0027ascii\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## 4. nn.Module.load_state_dict\n][\u003d-\n`model.load_state_dict(torch.load(\u0027.pth\u0027))` 会通过调用**每个子模块**的_load_from_state_dict 函数来加载他们所需的权重.\n而 _load_from_state_dict 才是真正负责加载 parameter 和 buffer 的函数.\n这也说明了每个模块可以自行定义他们的 _load_from_state_dict 函数来满足特殊需求.\n\n```\ndef _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n    # 获取模型的结构\n    local_name_params \u003d itertools.chain(self._parameters.items(), self._buffers.items()) \n    local_state \u003d {k: v.data for k, v in local_name_params if v is not None}\n    \n    # 对每一个结构参数进行加载\n    for name, param in local_state.items():\n        if isinstance(input_param, Parameter):  \n         # backwards compatibility for serialized parameters\n         \n         # 获取参数数据\n         input_param \u003d input_param.data\n         \n         # 加载数据tensor\n         try:\n            param.copy_(input_param)  \n\n```\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "##### 利用 load_from_state_dict来无痛加载迁移模型 [_load_from_state_dict](https://zhuanlan.zhihu.com/p/340453841)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                          missing_keys, unexpected_keys, error_msgs):\n    # override the _load_from_state_dict function\n    # convert the backbone weights pre-trained in Mask R-CNN\n    # use list(state_dict.keys()) to avoid\n    # RuntimeError: OrderedDict mutated during iteration\n    for key_name in list(state_dict.keys()):\n        key_changed \u003d True\n        if key_name.startswith(\u0027backbone.\u0027):\n            new_key_name \u003d f\u0027img_backbone{key_name[8:]}\u0027\n        elif key_name.startswith(\u0027neck.\u0027):\n            new_key_name \u003d f\u0027img_neck{key_name[4:]}\u0027\n        elif key_name.startswith(\u0027rpn_head.\u0027):\n            new_key_name \u003d f\u0027img_rpn_head{key_name[8:]}\u0027\n        elif key_name.startswith(\u0027roi_head.\u0027):\n            new_key_name \u003d f\u0027img_roi_head{key_name[8:]}\u0027\n        else:\n            key_changed \u003d False\n        if key_changed:\n            logger \u003d get_root_logger()\n            print_log(\n                f\u0027{key_name} renamed to be {new_key_name}\u0027, logger\u003dlogger)\n            state_dict[new_key_name] \u003d state_dict.pop(key_name)\n    super()._load_from_state_dict(state_dict, prefix, local_metadata,\n                                  strict, missing_keys, unexpected_keys,\n                                  error_msgs)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}